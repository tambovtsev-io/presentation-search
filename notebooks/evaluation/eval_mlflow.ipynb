{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Config, load_spreadsheet\n",
    "from src.rag import (\n",
    "    ChromaSlideStore,\n",
    "    HyperbolicScorer,\n",
    "    MinScorer,\n",
    "    PresentationRetriever,\n",
    "    ScorerTypes,\n",
    ")\n",
    "from src.eval.eval_mlflow import EvaluationConfig, RAGEvaluator\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mlflow setup logging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Setup components\n",
    "project_config = Config()\n",
    "llm = project_config.model_config.load_vsegpt(model=\"openai/gpt-4o-mini\")\n",
    "embeddings = project_config.embedding_config.load_vsegpt()\n",
    "embeddings = project_config.embedding_config.load_openai()\n",
    "\n",
    "storage = ChromaSlideStore(collection_name=\"pres0\", embedding_model=embeddings)\n",
    "\n",
    "db_path = project_config.navigator.processed / \"eval\" / \"runs\" / \"mlruns.db\"\n",
    "artifacts_path = project_config.navigator.processed / \"eval\" / \"artifacts\"\n",
    "eval_config = EvaluationConfig(\n",
    "    experiment_name=\"PresRetrieve_noanswer\",\n",
    "    metrics=[\"presentationmatch\", \"llmrelevance\"],\n",
    "    scorers=[MinScorer()],\n",
    "\n",
    ")\n",
    "\n",
    "evaluator = RAGEvaluator(\n",
    "    storage=storage,\n",
    "    config=eval_config,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pres_name</th>\n",
       "      <th>question</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Презентация про космонавтов</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Презентация про экономику Китая</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Зоомагазины</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The capital of Great Britain</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Обучение LLM на CPU</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Фото кабриолета</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>История Российской Империи</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pres_name                         question page  content  comment\n",
       "0        NaN      Презентация про космонавтов           NaN      NaN\n",
       "1        NaN  Презентация про экономику Китая           NaN      NaN\n",
       "2        NaN                      Зоомагазины           NaN      NaN\n",
       "3        NaN     The capital of Great Britain           NaN      NaN\n",
       "4        NaN              Обучение LLM на CPU           NaN      NaN\n",
       "5        NaN                  Фото кабриолета           NaN      NaN\n",
       "6        NaN       История Российской Империи           NaN      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "# Load questions\n",
    "sheet_id = os.environ[\"BENCHMARK_SPREADSHEET_ID\"]\n",
    "gids = {\n",
    "    \"NoAnswer\": \"1219206941\"\n",
    "}\n",
    "df = evaluator.load_questions_from_sheet(sheet_id, gid=gids[\"NoAnswer\"])\n",
    "\n",
    "df_eval = df.sample(5)\n",
    "df_eval = df.copy()\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run evaluation\n",
    "evaluator.run_evaluation(df_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
